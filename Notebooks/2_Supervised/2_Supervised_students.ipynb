{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HaMLeT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals of this session\n",
    "\n",
    "At the end of this session, you will have an understanding of the following:\n",
    "- Difference between supervised and unsupervised learning\n",
    "- What is meant by Classification in ML\n",
    "- Supervised Learning:\n",
    "    - Support Vector Machine (SVM)\n",
    "    - k-nearest neighbour\n",
    "    - Linear regression\n",
    "- Unsupervised Learning:\n",
    "    - k-means clustering\n",
    "    - GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most of the tasks in this notebook, we are going to use our knowledge about PCA from last session. So, instead of using the features of the Iris dataset directly, we are going to use the PCA features we calculated last time. It is advisable that you check the difference in results you obtain with the features directly versus with PCA for the tasks below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import some of the useful packages and the Iris dataset using `scikit`. We also perform PCA just as we did last time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "np.random.seed(seed=10) # to ensure reproducible reuslts\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data[:,[0,1,2,3]]\n",
    "targets = iris.target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data standarization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "features = StandardScaler().fit_transform(features)\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "sklearn_pca = sklearnPCA(n_components=2)\n",
    "features_PCA = sklearn_pca.fit_transform(features)\n",
    "\n",
    "species = ('Iris-setosa', 'Iris-versicolor', 'Iris-virginica')\n",
    "colors = ('blue','green','red')\n",
    "\n",
    "# Plotting the relationship between PCA1 and PCA2\n",
    "feature1 = features_PCA[:, 0] \n",
    "feature2 = features_PCA[:, 1] \n",
    "    \n",
    "species = ('Iris-setosa', 'Iris-versicolor', 'Iris-virginica')\n",
    "colors = ('blue','green','red')\n",
    "data = [[features_PCA[np.where(targets == target)][:, feature] for feature in [0, 1]] for target in range(3)]\n",
    "\n",
    "for item, color, group in zip(data, colors, species):\n",
    "    plt.scatter(item[0], item[1], color=color)\n",
    "    plt.title('Projection matrix')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.legend(species)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised Learning (Classification of the Iris dataset)\n",
    "\n",
    "Classification tasks aim at predicting the class of a given data sample. For the Iris dataset, we want to predict which one of the three species a given sample belongs to. In supervised learning, we do so by 'learning' a model based on some training samples. We then use this trained model to make predictions on 'unseen' or test-dataset.\n",
    "\n",
    "Before we dive into any of the algorithms, we need to get introduced to the idea of splitting the datset into train and test sets. \n",
    "\n",
    "We know that the Iris dataset consists of 150 samples. Now, we will take 80% of the samples as our training set and the remaining as test set. We calculate the accuracy of our model on the test set. The common ML terminology is as follows: X_train, X_test are, respectively, the feature vectors for training and testing; and y_train and y_test are the corresponding labels for X_train and X_test. The predictions will be stored in y_pred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Using the `train_test_split` function of `sklearn`, split the PCA features obtained above in accordance with the standard ML naming convention (as explained above). \n",
    "\n",
    "Tip: Please use the default parameters for the split. You will learn this topic more in details in the next session ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Hint: import the necessary modules first\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 My very own classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Refer to the PCA plot above and design your own classifier (by completing the assisting code!). For this, you may define simple linear or non-linear conditions to classify the sample points into one of the three classes. \n",
    "\n",
    "Example condition: if PCA1 for given sample < -2, sample is Iris-Setosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the following lines of code: \n",
    "\n",
    "class StudentsClassifier:\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_single(x) for x in X])\n",
    "    \n",
    "    def predict_single(self, X):\n",
    "        # X[0] = PCA feature 1\n",
    "        # X[1] = PCA feature 2\n",
    "        \n",
    "        # write a function that returns the index of the class\n",
    "        # e.g. if Iris-Setosa, return 0 \n",
    "        \n",
    "        # Your code here:\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Qualitative evaluation\n",
    "\n",
    "Below is a ready-to-use function to visualize classification boundaries. You are not required to understand it in detail but try to understand what inputs/outputs the functions require/produce. You will need to use these later in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(clf, X, y, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(constrained_layout=True)\n",
    "    \n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "    \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.7)\n",
    "    \n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=30, edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Using the function $plot\\_contours$, visualize the training boundaries obtained by your classifier by providing the correct arguments to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling your classifier\n",
    "my_clf = StudentsClassifier()\n",
    "# Visualizing class boundaries for my_clf\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Quantitative evaluation\n",
    "\n",
    "**Task:** First 'predict' the classes for X_test using the classifier you just defined above. And then calculate the accuracy on this test dataset. Use `scikit`'s `accuracy_score`.\n",
    "\n",
    "Hint: First import the necessary module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "my_clf = StudentsClassifier()\n",
    "\n",
    "# y_pred = ? # Complete the code!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Support Vector Machine\n",
    "\n",
    "SVMs are a powerful and flexible class of supervised algorithms for both classification and regression. They are memory efficient in that they use only a subset of the training points in the decision function (called support vectors). The simplest SVM uses a linear kernel to separate classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Using `sklearn`'s `svm`, implement a SVM with a linear kernel. Also perform a qualitative and a quantitative evaluation just as you did previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Example code: LinearSVC\n",
    "clf0 = svm.LinearSVC() # SVC = Support Vector Classifier\n",
    "# train clf0 on X_train and y_train\n",
    "clf0.fit(X_train, y_train)\n",
    "# visualize \n",
    "plot_contours(clf0, X_train, y_train)\n",
    "\n",
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How is it in comparison to the your classifier? (Compare the accuracies).\n",
    "\n",
    "Answer: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. More on accuracy metrics\n",
    "\n",
    "The Confusion Matrix provides a better summary of the classification performance than just the accuracy score. The latter is often not the best measure for classification tasks involving more than two classes. Calculating a confusion matrix can give you a better idea of what your classification model is getting right and what types of errors it is making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Read up on Confusion Matrix if you are not familiar with the term. Then using `sklearn`'s `confusion_matrix` draw it and try to understand the perfomance of your classifier. \n",
    "\n",
    "Tip: You may also want to look into `sklearn`'s `classification_report` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Tuning the Hyperparameters\n",
    "\n",
    "There are several parameters that can help achieve better results (introduced in the Preparatory Material). \n",
    "\n",
    "- Kernel: Depending on the (expected) distribution of our classes, we can choose different types of functions, eg. linear, polynomial, and radial basis function (RBF). As might be obvious, the latter two are useful for non-linear hyperplane.\n",
    "\n",
    "- Regularization: `C` in scikit-learn is a penalty parameter that controls the flexibility allowed to the hyperplane. A smaller value of C creates a small-margin hyperplane and a larger value creates a larger-margin hyperplane. \n",
    "\n",
    "- Gamma: This defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. A small gamma value define a Gaussian function with a large variance. In this case, two points can be considered similar even if are far from each other. In the other hand, a large gamma value means define a Gaussian function with a small variance and in this case, two points are considered similar just if they are close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** With a simple trial and error approach try to visualize and understand the effect of the hyperparameters and find an optimum classifier for the given dataset. You may use the metrices you learnt about to evaluate the performance of the different classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. k-nearest neighbour (knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knn Algorithm\n",
    "\n",
    "- Define a distance metric (Euclidean distance)\n",
    "- Choose a value for k (= the number of nearest neighbours) \n",
    "- Take k-nearest neighbors of the new data point, according to your distance metric\n",
    "- Assign the new data point the same category as its nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. My very own knn classifier\n",
    "\n",
    "We define our own knn classifier according to the above algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (Step 1):** Define a function that calculates the euclidean distance between two points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_ecd(v1, v2):\n",
    "    # Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (Step 2):** Define a function that uses my_ecd to calculate the distance between one single test data point with *all* the training data points. Save the distances *along with their respective indices* in a list. Return the *sorted* list as the output of the function. \n",
    "\n",
    "Hint: 1. To get indices, you may want to use `enumerate`.        2. You may use the function `sort` or `sorted`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_distance_metric(X_train, single_test):\n",
    "    \"\"\"Calculates the distance between one test sample X_test and every sample in X_train.\n",
    "\n",
    "    Parameters:\n",
    "    X_train = all available training samples\n",
    "    single_test = one particular test sample\n",
    "    k = number of nearest neighbours\n",
    "    -----------\n",
    "    Returns: sorted distance list  \n",
    "    \"\"\"\n",
    "    dist_list = []\n",
    "    # Your code here:\n",
    "    # Define a for-loop to compute distance\n",
    "\n",
    "    return dist_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Taks (Step 3):** Define a function to save the first k target values corresponding to dist_list obtained above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_target_list(dist_list, y_train, k): \n",
    "    # Your code here:\n",
    "    # make a list of the k neighbors' targets\n",
    "\n",
    "    return target_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (Step 4):** Define a function that assigns predictions to the test data points. \n",
    "\n",
    "Hint: Use `most_common`method from the `Counter` object to get the target that occurs maximum number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def my_predict(target_list):\n",
    "    # Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (Step 5):** Finally define a function that loops through all data points predicting each one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_knn(X_test, X_train, y_train, k):    \n",
    "    all_predictions = []\n",
    "    # Your code here:\n",
    "    # define a for-loop to loop through all the test data points \n",
    "    # to get the predictions for each one of them individually \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Use the knn-function\n",
    "to predict X_test and calculate the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Use the built-in knn classifier from `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How does your knn classifier perform in in comparison to the built-in function? \n",
    "\n",
    "Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unsupervised Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we always worked with features and targets (labels) of the given dataset. In unsupervised learning, we do not have any labels for the data. For classification task, here, we will rely on some clustering algorithms. \n",
    "\n",
    "In this section, we will see the K-means and Gausssian Mixture Model (GMM) clustering methods. These agorithms require us to 'guess' how many clusters (classes) we have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. K-means \n",
    "\n",
    "This is the simplest clustering algorithm. We initialize the algorithm with 'k' clusters according to which we get 'k' centroids. The algorithm then iteratively assigns every datapoint to its nearest cluster. The 'means' in its name refers to averaging of the data, i.e., finding the centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Perform unsupervised classification using `sklearn's` `KMeans`. To check your results, print out the output lables and compare with the target values. \n",
    "\n",
    "Note: For this task we ignore the several optional parameters. Providing only the `n_clusters` argument will suffice for this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Before you begin the task, think about which dataset should you work with here? \n",
    "\n",
    "Hint: Unsupervised learning = NO labels available\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Is it informative to compare the labels and the targets? Explain.\n",
    "\n",
    "Answer: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualization of labels\n",
    "\n",
    "I hope by now it is clear to you why calculating accuracy as done previously does not make sense in this case. Hence, we perform a simple visualization to assess the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Plot the results of the kmeans function as a scatter plot (similar to PCA). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "# Optional: Plotting the centroids of the clusters\n",
    "# plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 70, c = 'black')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The Elbow Method\n",
    "\n",
    "In a truly unsupervised learning scenario, how could we make the initial guess for the number of custers? One option is The Elbow Method.\n",
    "\n",
    "The basic idea behind cluster partitioning methods, such as k-means clustering, is to define clusters such that the total intra-cluster variation, or total within-cluster sum of square (wcss), is minimized. In the Elbow Method, we plot the WCSS against a set of values for 'k' and the location of a bend (elbow) in the plot is generally considered as an indicator of the appropriate number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the optimum number of clusters for k-means classification\n",
    "wcss = [] #within cluster sum of squares\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(features_PCA)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "#Plotting the results onto a line graph, allowing us to observe 'The elbow'\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the optimum number of clusters according to the Elbow Method? \n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 GMM\n",
    "\n",
    "As we saw, Kmeans might not always provide the most optimum output because it deos not have any intrinsic measure of probability or uncertainty of cluster assignments. A major limitation of k-means is that the cluster models must be circular: k-means has no built-in way of accounting for oblong or elliptical clusters.\n",
    "\n",
    "Gaussian mixture models (GMMs) offer an extension to the idea of kmeans and provide a better estimation. They attempt to find a mixture of multi-dimensional Gaussian probability distributions that best model any input dataset. While Kmeans is a method that performs hard labeling, i.e., it simply choses the maximum probability, GMM provide soft labeling by looking at all the probabilities instead of only maximum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Try out GMM using `sklearn`'s `GaussianMixture`. Display the probabilities that are assigned to every sample to understand the concept of soft-labeling as explained above. You may also plot the clusters for visualization. \n",
    "\n",
    "Tip: Round the probabilities up to two decimal places before displaying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bonus task \n",
    "\n",
    "**Task1:** For the above methods, it is highly interesting to work with the features directly without performing PCA. Compare performances of the methods with the results you obtained above with features_PCA.\n",
    "\n",
    "**Task2:** Perform a simple linear regression using `sklearn`'s built-in function on some randomly generated data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feedback Cell\n",
    "\n",
    "Hopefully you enjoyed this tutorial and learned some important classification methods used commonly in ML. Please leave your comments about what you liked/disliked in the session. Any suggestions are welcome!\n",
    "\n",
    "Your feedback:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
